<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
<div class="menu-item"><a href="home_teach.html">Teaching</a></div>
<div class="menu-item"><a href="home_services.html">Services</a></div>

<div class="menu-category">Research</div>
<div class="menu-item"><a href="research_publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="research_projects.html">Projects</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<h2>Conference</h2>
<!-- <h3>2025</h3> -->
<ol>
    <li>
        <p><a href="https://openaccess.thecvf.com/content/WACV2025/papers/Zhao_DualCIR_Enhancing_Training-Free_Composed_Image_Retrieval_via_Dual-Directional_Descriptions_WACV_2025_paper.pdf" target="_blank">DualCIR: Enhancing Training-Free Composed Image Retrieval via Dual-Directional Descriptions</a><br/>
    
        <span class="authors">Jingjiao Zhao, Jiaju Li, <span style="color:#BB2222">Dongze Lian</span>, Liguo Sun, Pin Lv</span> <br />

        IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2025.<br /></p>
    </li>

    <li>
        <p><a href="https://arxiv.org/pdf/2407.06964" target="_blank">Parameter-Eﬃcient and Memory-Eﬃcient Tuning for Vision Transformer: A Disentangled Approach</a><br/>
        
        <span class="authors">Taolin Zhang, Jiawang Bai, Zhihe Lu, <span style="color:#BB2222">Dongze Lian</span>, Genping Wang, Xinchao Wang, Shu-Tao Xia</span><br />

        European Conference on Computer Vision (<b>ECCV</b>), 2024.
        [<a href="https://synqt.github.io/" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://arxiv.org/pdf/2312.08746" target="_blank">DreamDrone: Text-to-Image Diﬀusion Models are Zero-shot Perpetual View Generators</a><br/>
        
        <span class="authors">Hanyang Kong, <span style="color:#BB2222">Dongze Lian</span>, Michael Bi Mi, Xinchao Wang</span><br />

        European Conference on Computer Vision (<b>ECCV</b>), 2024.<br /></p>
    </li>

    <li>
        <p><a href="https://openreview.net/pdf?id=HGSIpeNNfM" target="_blank">Receptive Fields As Experts in Convolutional Neural Networks</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Weihao Yu, Xinchao Wang</span><br />

        International Conference on Machine Learning (<b>ICML</b>), 2024.<br /></p>
    </li>
    
    <li>
        <p><a href="https://openaccess.thecvf.com/content/WACV2024/papers/Wang_TSP-Transformer_Task-Specific_Prompts_Boosted_Transformer_for_Holistic_Scene_Understanding_WACV_2024_paper.pdf" target="_blank">TSP-Transformer: Task-Specific Prompts Boosted Transformer for Holistic Scene Understanding</a><br/>
        
        <span class="authors">Shuo Wang, Jing Li, Zibo Zhao, <span style="color:#BB2222">Dongze Lian</span>, Binbin Huang, Xiaomei Wang, Zhengxin Li, Shenghua Gao</span><br />

        IEEE Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2024.
        [<a href="https://github.com/tb2-sy/TSP-Transformer" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openreview.net/pdf?id=YmEDnMynuO" target="_blank">GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph</a><br/>
        
        <span class="authors">Xin Li, <span style="color:#BB2222">Dongze Lian (Co-1st)</span>, Zhihe Lu, Jiawang Bai, Zhibo Chen, Xinchao Wang</span><br />

        Neural Information Processing Systems (<b>NeurIPS</b>), 2023.
        [<a href="https://github.com/lixinustc/GraphAdapter" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.pdf" target="_blank">Priority-Centric Human Motion Generation in Discrete Latent Space</a><br/>
        
        <span class="authors">Hanyang Kong, Kehong Gong, <span style="color:#BB2222">Dongze Lian</span>, Michael Bi Mi, Xinchao Wang</span><br />

        IEEE International Conference on Computer Vision (<b>ICCV</b>), 2023.<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Dataset_Quantization_ICCV_2023_paper.pdf" target="_blank">Dataset Quantization</a><br/>
        
        <span class="authors">Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, <span style="color:#BB2222">Dongze Lian</span>, Yifan Zhang, Yang You, Jiashi Feng</span><br />

        IEEE International Conference on Computer Vision (<b>ICCV</b>), 2023.
        [<a href="https://github.com/magic-research/Dataset_Quantization" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_TM2D_Bimodality_Driven_3D_Dance_Generation_via_Music-Text_Integration_ICCV_2023_paper.pdf" target="_blank">TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration</a><br/>
        
        <span class="authors">Kehong Gong, <span style="color:#BB2222">Dongze Lian (Co-1st)</span>, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, Xinchao Wang</span><br />

        IEEE International Conference on Computer Vision (<b>ICCV</b>), 2023.
        [<a href="https://garfield-kh.github.io/TM2D/" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://arxiv.org/pdf/2307.12558" target="_blank">Revisiting Event-based Video Frame Interpolation</a><br/>
        
        <span class="authors">Jiaben Chen, Yichen Zhu, <span style="color:#BB2222">Dongze Lian</span>, Jiaqi Yang, Yifu Wang, Renrui Zhang, Xinhang Liu, Shenhan Qian, Laurent Kneip, Shenghua Gao</span><br />

        IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2023.
        [<a href="https://jiabenchen.github.io/revisit_event" target="_blank">Website</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Dong_Weakly_Supervised_Video_Representation_Learning_With_Unaligned_Text_for_Sequential_CVPR_2023_paper.pdf" target="_blank">Weakly Supervised Video Representation Learning With Unaligned Text for Sequential Videos</a><br/>
        
        <span class="authors">Sixun Dong, Huazhang Hu, <span style="color:#BB2222">Dongze Lian</span>, Weixin Luo, Yicheng Qian, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.
        [<a href="https://github.com/svip-lab/WeakSVR" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf" target="_blank">iQuery: Instruments As Queries for Audio-Visual Sound Separation</a><br/>
        
        <span class="authors">Jiaben Chen, Renrui Zhang, <span style="color:#BB2222">Dongze Lian</span>, Jiaqi Yang, Ziyao Zeng, Jianbo Shi</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2023.
        [<a href="https://jiabenchen.github.io/iQuery/" target="_blank">Website</a>] [<a href="https://github.com/JiabenChen/iQuery" target="">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://papers.neurips.cc/paper_files/paper/2022/file/00bb4e415ef117f2dee2fc3b778d806d-Paper-Conference.pdf" target="_blank">Scaling & Shifting Your Features: A New Baseline for Eﬃcient Model Tuning</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Daquan Zhou, Jiashi Feng, Xinchao Wang</span><br />

        Neural Information Processing Systems (<b>NeurIPS</b>), 2022. [<a href="https://github.com/dongzelian/SSF" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Qian_SVIP_Sequence_VerIfication_for_Procedures_in_Videos_CVPR_2022_paper.pdf" target="_blank">SVIP: Sequence VerIfication for Procedures in Videos</a><br/>
        
        <span class="authors">Yicheng Qian, Weixin Luo, <span style="color:#BB2222">Dongze Lian</span>, Xu Tang, Peilin Zhao, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022. [<a href="https://github.com/svip-lab/SVIP-Sequence-VerIfication-for-Procedures-in-Videos" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hu_TransRAC_Encoding_Multi-Scale_Temporal_Correlation_With_Transformers_for_Repetitive_Action_CVPR_2022_paper.pdf" target="_blank">TransRAC: Encoding Multi-Scale Temporal Correlation With Transformers for Repetitive Action Counting</a><br/>
        
        <span class="authors">Huazhang Hu, Sixun Dong, Yiqun Zhao, <span style="color:#BB2222">Dongze Lian</span>, Zhengxin Li, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR, Oral</b>), 2022. [<a href="https://github.com/SvipRepetitionCounting/TransRAC" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://arxiv.org/pdf/2107.08391.pdf" target="_blank">AS-MLP: An Axial Shifted MLP Architecture for Vision</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Zehao Yu, Xing Sun, Shenghua Gao</span><br />

        International Conference on Learning Representations (<b>ICLR</b>), 2022. 
        [<a href="https://github.com/svip-lab/AS-MLP" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_Crowd_Counting_With_Partial_Annotations_in_an_Image_ICCV_2021_paper.pdf" target="_blank">Crowd Counting With Partial Annotations in an Image</a><br/>
        
        <span class="authors">Yanyu Xu, Ziming Zhong, <span style="color:#BB2222">Dongze Lian</span>, Jing Li, Zhengxin Li, Xinxing Xu, Shenghua Gao</span><br />

        IEEE International Conference on Computer Vision (<b>ICCV</b>), 2021.
        [<a href="https://github.com/svip-lab/CrowdCountingPAL" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Look_Before_You_Leap_Learning_Landmark_Features_for_One-Stage_Visual_CVPR_2021_paper.pdf" target="_blank">Look Before You Leap: Learning Landmark Features for One-Stage Visual Grounding</a><br/>
        
        <span class="authors">Binbin Huang, <span style="color:#BB2222">Dongze Lian</span>, Weixin Luo, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021.
        [<a href="https://github.com/svip-lab/LBYLNet" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16346" target="_blank">KGDet: Keypoint-Guided Fashion Detection</a><br/>
        
        <span class="authors">Shenhan Qian, <span style="color:#BB2222">Dongze Lian (Co-1st)</span>, Binqiang Zhao, Tong Liu, Bohui Zhu, Hai Li, Shenghua Gao</span><br />

        AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021.
        [<a href="https://github.com/ShenhanQian/KGDet" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openreview.net/pdf?id=r1eowANFvr" target="_blank">Towards Fast Adaptation of Neural Architectures with Meta Learning</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, Shenghua Gao</span><br />

        International Conference on Learning Representations (<b>ICLR</b>), 2020.
        [<a href="https://github.com/dongzelian/T-NAS" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Lian_Density_Map_Regression_Guided_Detection_Network_for_RGB-D_Crowd_Counting_CVPR_2019_paper.pdf" target="_blank">Density Map Regression Guided Detection Network for RGB-D Crowd Counting and Localization</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Jing Li, Jia Zheng, Weixin Luo, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.
        [<a href="https://github.com/svip-lab/RGBD-Counting" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Cheng_Local_to_Global_Learning_Gradually_Adding_Classes_for_Training_Deep_CVPR_2019_paper.pdf" target="_blank">Local to Global Learning: Gradually Adding Classes for Training Deep Neural Networks</a><br/>
        
        <span class="authors">Hao Cheng, <span style="color:#BB2222">Dongze Lian (Co-1st)</span>, Bowen Deng, Shenghua Gao, Tao Tan, Yanlin Geng</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.
        [<a href="https://github.com/piratehao/Local-to-Global-Learning-for-DNNs" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Single-Image_Piece-Wise_Planar_3D_Reconstruction_via_Associative_Embedding_CVPR_2019_paper.pdf" target="_blank">Single-Image Piece-wise Planar 3D Reconstruction via Associative Embedding</a><br/>
        
        <span class="authors">Zehao Yu, Jia Zheng, <span style="color:#BB2222">Dongze Lian</span>, Zihan Zhou, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.
        [<a href="https://github.com/svip-lab/PlanarReconstruction" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://ojs.aaai.org//index.php/AAAI/article/view/4094" target="_blank">RGBD Based Gaze Estimation via Multi-task CNNs</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Ziheng Zhang, Weixin Luo, Lina Hu, Minye Wu, Zechao Li, Jingyi Yu, Shenghua Gao</span><br />

        AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2019.
        [<a href="https://github.com/svip-lab/RGBD-Gaze" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Hao_Cheng_Evaluating_Capability_of_ECCV_2018_paper.pdf" target="_blank">Evaluating Capability of Deep Neural Networks for Image Classification via Mutual Information</a><br/>
        
        <span class="authors">Hao Cheng, <span style="color:#BB2222">Dongze Lian</span>, Shenghua Gao, Yanlin Geng</span><br />

        European Conference on Computer Vision (<b>ECCV</b>), 2018.<br /></p>
    </li>

    <li>
        <p><a href="https://arxiv.org/pdf/1907.02364.pdf" target="_blank">Believe It or Not, We Know What You Are Looking at!</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Zehao Yu, Shenghua Gao</span><br />

        Asian Conference on Computer Vision (<b>ACCV, Oral</b>), 2018.
        [<a href="https://github.com/svip-lab/GazeFollowing" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Future_Frame_Prediction_CVPR_2018_paper.pdf" target="_blank">Future Frame Prediction for Anomaly Detection – A New Baseline</a><br/>
        
        <span class="authors">Wen Liu, Weixin Luo, <span style="color:#BB2222">Dongze Lian</span>, Shenghua Gao</span><br />

        IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018.
        [<a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018" target="_blank">Code</a>]<br /></p>
    </li>

</ol>

<h2>Journal</h2>
<ol>
    <li>
        <p><a href="https://ieeexplore.ieee.org/abstract/document/9601215/" target="_blank">Locating and Counting Heads in Crowds With a Depth Prior</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Xianing Chen, Jing Li, Weixin Luo, and Shenghua Gao</span><br />

        IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021.
        [<a href="https://github.com/svip-lab/RGBD-Counting" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://ieeexplore.ieee.org/abstract/document/9622181/" target="_blank">Future Frame Prediction Network for Video Anomaly Detection</a><br/>
        
        <span class="authors">Weixin Luo, Wen Liu, <span style="color:#BB2222">Dongze Lian</span>, and Shenghua Gao</span><br />

        IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2021.<br /></p>
    </li>

    <li>
        <p><a href="https://ieeexplore.ieee.org/document/8851288" target="_blank">Video Anomaly Detection with Sparse Coding Inspired Deep Neural Networks</a><br/>
        
        <span class="authors">Weixin Luo, Wen Liu, <span style="color:#BB2222">Dongze Lian</span>, Jinhui Tang, Lixin Duan, Xi Peng, Shenghua Gao</span><br />

        IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), 2019.
        [<a href="https://github.com/StevenLiuWen/sRNN_TSC_Anomaly_Detection" target="_blank">Code</a>]<br /></p>
    </li>

    <li>
        <p><a href="https://www.researchgate.net/publication/332805537_Utilizing_Information_Bottleneck_to_Evaluate_the_Capability_of_Deep_Neural_Networks_for_Image_Classification" target="_blank">Utilizing Information Bottleneck to Evaluate the Capability of Deep Neural Networks for Image Classification</a><br/>
        
        <span class="authors">Hao Cheng, <span style="color:#BB2222">Dongze Lian</span>, Shenghua Gao, Yanlin Geng</span><br />

        Entropy (<b>Entropy</b>), 2019.<br /></p>
    </li>

    <li>
        <p><a href="https://ieeexplore.ieee.org/document/8454246/" target="_blank">Multi-view Multi-task Gaze Prediction with Deep Convolutional Neural Networks</a><br/>
        
        <span class="authors"><span style="color:#BB2222">Dongze Lian</span>, Lina Hu, Weixin Luo, Yanyu Xu, Lixin Duan, Jingyi Yu, Shenghua Gao</span><br />

        IEEE Transactions on Neural Networks and Learning Systems (<b>TNNLS</b>), 2018.
        [<a href="https://github.com/dongzelian/multi-view-gaze" target="_blank">Code</a>]<br /></p>
    </li>

</ol>

</td>
</tr>
</table>
</body>
</html>
